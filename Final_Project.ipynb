{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load & Preprocessing Function Description**  \n",
    "  \n",
    "1. Read tweets\n",
    "2. Extract text and sentiment label\n",
    "3. Tokenize, lower-case, lemmatize the tokens, then exclude punctuations, pure numbers and web links.\n",
    "4. Collect the processed tweeter text as a list of token list + sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Filters used in the main function below\n",
    "def filter_tokens(tokens):\n",
    "    pattern1 = re.compile(r'^(https?://[^\\s]+$|[^\\w\\s])')\n",
    "    pattern2 = re.compile(r'\\d+')\n",
    "    # Exclude punctuations and web links\n",
    "    filtered_ts = [token for token in tokens if not pattern1.match(token)]\n",
    "    filtered_ts= [token for token in filtered_ts if not pattern2.match(token)]\n",
    "    return filtered_ts\n",
    "def sw_filter(tokens):\n",
    "    # stopwords provided by 'TweetData'\n",
    "    stopwords = [line.strip() for line in open('TweetData/stopwords_twitter.txt')]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Main function\n",
    "def processtweets(path,nosw):\n",
    "\n",
    "    # initialize NLTK built-in tweet tokenizer\n",
    "    twtokenizer = TweetTokenizer()\n",
    "    # read file\n",
    "    f = open(path, 'r')\n",
    "\n",
    "    # gather the original data\n",
    "    tweetdata = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # each line has 4 items separated by tabs\n",
    "        # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "        tweetdata.append(line.split('\\t')[2:4])\n",
    "\n",
    "    # create list of tweet documents as (list of words, label)\n",
    "    # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "    # Create a list for the data\n",
    "    tweetdocs = []\n",
    "    # add all the tweets except the ones whose text is Not Available\n",
    "    neg_num=0\n",
    "    pos_num=0\n",
    "    neu_num=0\n",
    "    for tweet in tweetdata:\n",
    "        if (tweet[1] != 'Not Available'):\n",
    "            # tokenize each tweet text\n",
    "            tokens = twtokenizer.tokenize(tweet[1])\n",
    "            \n",
    "\n",
    "            # and used lemmatizer on them\n",
    "            tokens_lower=[token.lower() for token in tokens]\n",
    "            text=nltk.Text(tokens_lower)\n",
    "            wnl = nltk.WordNetLemmatizer()\n",
    "            tokens_lemma=[wnl.lemmatize(t) for t in text]\n",
    "            # Then filter out web pages and pure numbers and punctuations\n",
    "            tokens_filtered=filter_tokens(tokens_lemma)\n",
    "\n",
    "            # if we choose to exclude stop words\n",
    "            if nosw==1:\n",
    "                tokens_filtered=sw_filter(tokens_filtered)\n",
    "            \n",
    "            if tweet[0] == '\"negative\"':\n",
    "                label = 'neg'\n",
    "                neg_num+=1\n",
    "            elif tweet[0] == '\"positive\"':\n",
    "                label = 'pos'\n",
    "                pos_num+=1\n",
    "            else:\n",
    "                label='neu'\n",
    "                neu_num+=1\n",
    "\n",
    "            # add tokens, label to our document list\n",
    "            tweetdocs.append((tokens_filtered, label))\n",
    "    return [tweetdocs,pos_num,neg_num,neu_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3059, 1207, 3942]\n",
      "[491, 290, 632]\n",
      "8208\n",
      "1413\n",
      "['last', 'day', 'in', 'jeddah', 'will', 'be', 'in', 'brunei', 'tomorrow', 'night', 'and', 'then', 'surabaya', 'the', 'following', 'night', 'and', 'then', 'bali', 'the', 'night', 'after', 'that', 'whee']\n",
      "neu\n",
      "['last', 'day', 'jeddah', 'will', 'brunei', 'tomorrow', 'night', 'surabaya', 'following', 'night', 'bali', 'night', 'whee']\n",
      "neu\n"
     ]
    }
   ],
   "source": [
    "#In a.dev.dist.tsv are just ids, pure numbers of tweets, not useful.\n",
    "# b-dist can be seen as a train set while b.dev.dist is a separate test set we can use later.\n",
    "import random\n",
    "train_path='TweetData/corpus/downloaded-tweeti-b-dist.tsv'\n",
    "test_path='TweetData/corpus/downloaded-tweeti-b.dev.dist.tsv'\n",
    "\n",
    "# print number of tweets in each group\n",
    "train_documents,pos_num,neg_num,neu_num=processtweets(train_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "test_documents,pos_num,neg_num,neu_num=processtweets(test_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "\n",
    "train_documents_nosw=processtweets(train_path,1)[0]\n",
    "test_documents_nosw=processtweets(test_path,1)[0]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents)\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents_nosw)\n",
    "\n",
    "# Print the amount of tweets\n",
    "print(len(train_documents))\n",
    "print(len(test_documents))\n",
    "\n",
    "# Print the tokenized tweet and label of the thrid document\n",
    "print(train_documents[2][0])\n",
    "print(train_documents[2][1])\n",
    "print(train_documents_nosw[2][0])\n",
    "print(train_documents_nosw[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral tweets are the most, followed by positive ones and negative ones are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word features generating and selection**  \n",
    "  \n",
    "1. Create word features using top half of frequent words\n",
    "2. Calculate baseline accuracy\n",
    "3. 10-fold cross validation, for each time, record the most informative 100 words\n",
    "4. Gather the 10 100-words sets together as the useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14060 unique words in total\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in train_documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print('There are',len(all_words),'unique words in total')\n",
    "word_items = all_words.most_common(int(0.5*len(all_words)))\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_{}'.format(word)] = (word in document_words)\n",
    "\treturn features\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62875\n"
     ]
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "train_set, test_set = featuresets[800:], featuresets[:800]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cross validation function, after regular printing accuracy, confusion matrix, precision, recall and F-1 scores, the most informative 100 word features each round are collected as a set. \n",
    "  \n",
    "We will use those word features for further classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # confusion matrix\n",
    "    cm = nltk.ConfusionMatrix(gold, predicted)\n",
    "    print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))\n",
    "\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    word_set=set()\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:(i+1)*subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "        # train using train_this_round\n",
    "        classifier_this_round = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        infeatures=classifier_this_round.most_informative_features(100)\n",
    "        for item in infeatures:\n",
    "            word=item[0][2:]\n",
    "            word_set.add(word)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier_this_round, test_this_round)\n",
    "        print(i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "\n",
    "        # predicted and test \n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier_this_round.classify(features))\n",
    "\n",
    "        # print confusion matrix and evaluating measures\n",
    "        eval_measures(goldlist,predictedlist)\n",
    "\n",
    "    \n",
    "    # find mean accuracy over all rounds\n",
    "    print('mean accuracy', sum(accuracy_list) / num_folds)\n",
    "    return list(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6191346739792809\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<491>198  85 |\n",
      "pos | 156<404> 64 |\n",
      "neg |  57  65<121>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.647      0.606      0.626\n",
      "neu \t      0.634      0.697      0.664\n",
      "neg \t      0.498      0.448      0.472\n",
      "1 0.6514320536258379\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<565>191  68 |\n",
      "pos | 133<404> 36 |\n",
      "neg |  76  68<100>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.705      0.609      0.654\n",
      "neu \t      0.686      0.730      0.707\n",
      "neg \t      0.410      0.490      0.446\n",
      "2 0.6179159049360147\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<543>185  82 |\n",
      "pos | 160<380> 41 |\n",
      "neg |  84  75 <91>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.654      0.594      0.622\n",
      "neu \t      0.670      0.690      0.680\n",
      "neg \t      0.364      0.425      0.392\n",
      "3 0.6179159049360147\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<504>186  61 |\n",
      "pos | 184<408> 44 |\n",
      "neg |  70  82<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.642      0.604      0.622\n",
      "neu \t      0.671      0.665      0.668\n",
      "neg \t      0.402      0.493      0.443\n",
      "4 0.6544789762340036\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<550>178  53 |\n",
      "pos | 169<441> 35 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.684      0.649      0.666\n",
      "neu \t      0.704      0.696      0.700\n",
      "neg \t      0.386      0.485      0.430\n",
      "mean accuracy 0.6321755027422303\n",
      "['breakout', 'window', 'crap', 'free', 'fail', 'fuckin', 'wait', 'blame', 'alone', 'poetry', 'coz', 'except', 'madonna', 'bored', 'nice', 'cry', 'evil', 'rejected', 'andrew', 'penalty', 'fl', 'problem', 'khan', 'emerson', 'report', 'sick', 'dropping', 'surprised', 'hopefully', 'afghan', 'sad', 'assist', 'forward', 'hell', 'storm', 'love', 'thanks', 'fucked', 'demitra', \"wouldn't\", 'parade', 'ceo', 'yay', 'failed', 'mnf', 'protest', 'suppose', 'error', 'amazing', 'worst', 'bullshit', 'adnan', 'delay', 'knew', 'bro', 'smh', 'brilliant', 'rafa', 'gop', 'leg', 'attention', 'final', 'sense', 'policy', \"can't\", \"couldn't\", 'rohypnol', 'kill', 'boehner', 'cif', 'h', 'marijuana', 'jenelle', 'award', 'thank', 'happy', 'joke', 'serious', 'changed', 'international', 'rudd', 'leaf', 'tatum', 'luck', 'fuck', 'rookie', 'caltrain', 'doesnt', 'bad', 'juice', 'cancelled', 'beating', 'seem', 'six', 'computer', 'warned', 'possibly', 'channing', 'bellamy', 'hawaii', 'exciting', 'sb', 'interesting', 'great', 'wrong', 'favorite', 'injury', 'khl', 'killed', 'weekend', 'cuz', 'trayvon', 'wont', 'instead', 'service', 'loss', 'drug', 'mad', 'mistake', 'bench', 'body', 'swift', 'dwts', 'knocked', 'lb', 'shit', 'lovely', 'dying', 'scotland', 'pavol', 'plane', \"harry's\", 'fantastic', 'outside', 'fucking', 'suck', 'spirit', 'dick', 'poll', 'tired', 'canceled', 'trial', 'anarchy', 'via', \"ain't\", 'kit', 'concert', 'johnny', 'net', 'notebook', 'missing', 'meant', 'breitbart', 'liked', 'die', 'absolutely', 'iebc', 'as', 'matter', 'sorry', 'funny', 'voice', 'cliff', 'decided', 'hate', 'factor', 'best', 'nothing', 'excuse', 'fun', 'good', 'august', 'language', 'stevie', 'live', 'score', 'bless', 'losing', 'damn', 'netanyahu', 'potus', 'anymore', 'kinda', 'rider', 'perfect', 'twat', 'lucas', 'debate', 'deal', \"didn't\", 'fit', 'nooooooooo', \"hasn't\", 'enjoy', 'dont', 'ticket', 'cool', 'fear', 'throw', 'believe', 'awesome', 'excited', 'option', 'compared', 'avril', 'bitch', 'crash', 'dead', 'why', 'happened', 'enjoying', 'cant', 'horse']\n"
     ]
    }
   ],
   "source": [
    "word_feature_final=cross_validation(5,featuresets)\n",
    "print(word_feature_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "['breakout', 'window', 'crap', 'free', 'fail', 'fuckin', 'wait', 'blame', 'alone', 'poetry']\n",
      "['coz', 'except', 'madonna', 'bored', 'nice', 'cry', 'evil', 'rejected', 'andrew', 'penalty']\n",
      "['fl', 'problem', 'khan', 'emerson', 'report', 'sick', 'dropping', 'surprised', 'hopefully', 'afghan']\n"
     ]
    }
   ],
   "source": [
    "print(len(word_feature_final))\n",
    "print(word_feature_final[:10])\n",
    "print(word_feature_final[10:20])\n",
    "print(word_feature_final[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base line result is not good, I believe this is partly because of the limited data observations. Neutral tweets classification get better scores than positive ones, the negative ones have the worst scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram features generating and selection**  \n",
    "  \n",
    "1. Create bigram features using top 1000 best bigrams measuered by chi square\n",
    "2. Calculate baseline accuracy\n",
    "3. 10-fold cross validation, for each time, record the most informative 100 bigrams\n",
    "4. Gather the 10 100-bigram sets together as the useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 1000)\n",
    "\n",
    "def bigram_document_features(document, bigram_features):\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "0.47073170731707314\n"
     ]
    }
   ],
   "source": [
    "bigram_featuresets = [(bigram_document_features(d,bigram_features), c) for (d,c) in train_documents]\n",
    "thresh=int(len(bigram_featuresets)*0.1)\n",
    "print(thresh)\n",
    "train_set, test_set = bigram_featuresets[thresh:], bigram_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<386>  .   . |\n",
      "pos | 318  <.>  . |\n",
      "neg | 116   .  <.>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(classifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the bigrams won't give any valuable information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tagged features**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n",
    "\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6365853658536585"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=int(len(POS_featuresets)*0.1)\n",
    "train_set, test_set = POS_featuresets[thresh:], POS_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6288848263254113\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<503>184  87 |\n",
      "pos | 146<405> 73 |\n",
      "neg |  54  65<124>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.649      0.619      0.634\n",
      "neu \t      0.650      0.716      0.681\n",
      "neg \t      0.510      0.437      0.471\n",
      "1 0.6550883607556368\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<569>177  78 |\n",
      "pos | 135<401> 37 |\n",
      "neg |  74  65<105>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.700      0.624      0.660\n",
      "neu \t      0.691      0.731      0.710\n",
      "neg \t      0.430      0.477      0.453\n",
      "2 0.6227909811090798\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<548>177  85 |\n",
      "pos | 168<372> 41 |\n",
      "neg |  73  75<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.640      0.596      0.617\n",
      "neu \t      0.677      0.695      0.685\n",
      "neg \t      0.408      0.447      0.427\n",
      "3 0.6215722120658135\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<510>169  72 |\n",
      "pos | 186<404> 46 |\n",
      "neg |  69  79<106>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.635      0.620      0.627\n",
      "neu \t      0.679      0.667      0.673\n",
      "neg \t      0.417      0.473      0.444\n",
      "4 0.6514320536258379\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<556>168  57 |\n",
      "pos | 177<430> 38 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.667      0.653      0.660\n",
      "neu \t      0.712      0.692      0.702\n",
      "neg \t      0.386      0.466      0.422\n",
      "mean accuracy 0.6359536867763558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_feature_final=cross_validation(5,POS_featuresets)\n",
    "len(POS_feature_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion at this stage**  \n",
    "  \n",
    "For now, we did preprocessing on the data, tried classification with Naive Bayes model on the tokenized tweets using word-only, bigram, and POS-tagged features. We collected the most useful several hundred words for further classification tasks.  \n",
    "  \n",
    "We found:  \n",
    "1. Bigram features are found not valuable for classification\n",
    "2. POS-tagged features have slightly better result, compared with words-only ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some plans for next steps:**\n",
    "  \n",
    "1. Check the scores of no-stop-words version of the document (already made as \"train_document_nosw\")\n",
    "2. Try using some sentiment score APIs on the words so that we get sentiment values as features.\n",
    "3. Try other models for this classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
