{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load & Preprocessing Function Description**  \n",
    "  \n",
    "1. Read tweets\n",
    "2. Extract text and sentiment label\n",
    "3. Tokenize, lower-case, lemmatize the tokens, then exclude punctuations, pure numbers and web links.\n",
    "4. Collect the processed tweeter text as a list of token list + sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Filters used in the main function below\n",
    "def filter_tokens(tokens):\n",
    "    pattern1 = re.compile(r'^(https?://[^\\s]+$|[^\\w\\s])')\n",
    "    pattern2 = re.compile(r'\\d+')\n",
    "    # Exclude punctuations and web links\n",
    "    filtered_ts = [token for token in tokens if not pattern1.match(token)]\n",
    "    filtered_ts= [token for token in filtered_ts if not pattern2.match(token)]\n",
    "    return filtered_ts\n",
    "def sw_filter(tokens):\n",
    "    # stopwords provided by 'TweetData'\n",
    "    stopwords = [line.strip() for line in open('TweetData/stopwords_twitter.txt')]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Main function\n",
    "def processtweets(path,nosw):\n",
    "\n",
    "    # initialize NLTK built-in tweet tokenizer\n",
    "    twtokenizer = TweetTokenizer()\n",
    "    # read file\n",
    "    f = open(path, 'r')\n",
    "\n",
    "    # gather the original data\n",
    "    tweetdata = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # each line has 4 items separated by tabs\n",
    "        # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "        tweetdata.append(line.split('\\t')[2:4])\n",
    "\n",
    "    # create list of tweet documents as (list of words, label)\n",
    "    # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "    # Create a list for the data\n",
    "    tweetdocs = []\n",
    "    # add all the tweets except the ones whose text is Not Available\n",
    "    neg_num=0\n",
    "    pos_num=0\n",
    "    neu_num=0\n",
    "    for tweet in tweetdata:\n",
    "        if (tweet[1] != 'Not Available'):\n",
    "            # tokenize each tweet text\n",
    "            tokens = twtokenizer.tokenize(tweet[1])\n",
    "            \n",
    "\n",
    "            # and used lemmatizer on them\n",
    "            tokens_lower=[token.lower() for token in tokens]\n",
    "            text=nltk.Text(tokens_lower)\n",
    "            wnl = nltk.WordNetLemmatizer()\n",
    "            tokens_lemma=[wnl.lemmatize(t) for t in text]\n",
    "            # Then filter out web pages and pure numbers and punctuations\n",
    "            tokens_filtered=filter_tokens(tokens_lemma)\n",
    "\n",
    "            # if we choose to exclude stop words\n",
    "            if nosw==1:\n",
    "                tokens_filtered=sw_filter(tokens_filtered)\n",
    "            \n",
    "            if tweet[0] == '\"negative\"':\n",
    "                label = 'neg'\n",
    "                neg_num+=1\n",
    "            elif tweet[0] == '\"positive\"':\n",
    "                label = 'pos'\n",
    "                pos_num+=1\n",
    "            else:\n",
    "                label='neu'\n",
    "                neu_num+=1\n",
    "\n",
    "            # add tokens, label to our document list\n",
    "            tweetdocs.append((tokens_filtered, label))\n",
    "    return [tweetdocs,pos_num,neg_num,neu_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3059, 1207, 3942]\n",
      "[491, 290, 632]\n",
      "8208\n",
      "1413\n",
      "['last', 'day', 'in', 'jeddah', 'will', 'be', 'in', 'brunei', 'tomorrow', 'night', 'and', 'then', 'surabaya', 'the', 'following', 'night', 'and', 'then', 'bali', 'the', 'night', 'after', 'that', 'whee']\n",
      "neu\n",
      "['last', 'day', 'jeddah', 'will', 'brunei', 'tomorrow', 'night', 'surabaya', 'following', 'night', 'bali', 'night', 'whee']\n",
      "neu\n"
     ]
    }
   ],
   "source": [
    "#In a.dev.dist.tsv are just ids, pure numbers of tweets, not useful.\n",
    "# b-dist can be seen as a train set while b.dev.dist is a separate test set we can use later.\n",
    "import random\n",
    "train_path='TweetData/corpus/downloaded-tweeti-b-dist.tsv'\n",
    "test_path='TweetData/corpus/downloaded-tweeti-b.dev.dist.tsv'\n",
    "\n",
    "# print number of tweets in each group\n",
    "train_documents,pos_num,neg_num,neu_num=processtweets(train_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "test_documents,pos_num,neg_num,neu_num=processtweets(test_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "\n",
    "train_documents_nosw=processtweets(train_path,1)[0]\n",
    "test_documents_nosw=processtweets(test_path,1)[0]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents)\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents_nosw)\n",
    "\n",
    "# Print the amount of tweets\n",
    "print(len(train_documents))\n",
    "print(len(test_documents))\n",
    "\n",
    "# Print the tokenized tweet and label of the thrid document\n",
    "print(train_documents[2][0])\n",
    "print(train_documents[2][1])\n",
    "print(train_documents_nosw[2][0])\n",
    "print(train_documents_nosw[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral tweets are the most, followed by positive ones and negative ones are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word features generating and selection**  \n",
    "  \n",
    "1. Create word features using top half of frequent words\n",
    "2. Calculate baseline accuracy\n",
    "3. 5-fold cross validation, for each time, record the most informative 100 words\n",
    "4. Gather the 10 100-words sets together as the most useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14060 unique words in total\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in train_documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print('There are',len(all_words),'unique words in total')\n",
    "word_items = all_words.most_common(int(0.5*len(all_words)))\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_{}'.format(word)] = (word in document_words)\n",
    "\treturn features\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62875\n"
     ]
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "train_set, test_set = featuresets[800:], featuresets[:800]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cross validation function, after regular printing accuracy, confusion matrix, precision, recall and F-1 scores, the most informative 100 word features each round are collected as a python set. \n",
    "  \n",
    "We will use those word features for further classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # confusion matrix\n",
    "    cm = nltk.ConfusionMatrix(gold, predicted)\n",
    "    print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))\n",
    "\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    word_set=set()\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:(i+1)*subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "        # train using train_this_round\n",
    "        classifier_this_round = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        infeatures=classifier_this_round.most_informative_features(100)\n",
    "        for item in infeatures:\n",
    "            word=item[0][2:]\n",
    "            word_set.add(word)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier_this_round, test_this_round)\n",
    "        print(i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "\n",
    "        # predicted and test \n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier_this_round.classify(features))\n",
    "\n",
    "        # print confusion matrix and evaluating measures\n",
    "        eval_measures(goldlist,predictedlist)\n",
    "\n",
    "    \n",
    "    # find mean accuracy over all rounds\n",
    "    print('mean accuracy', sum(accuracy_list) / num_folds)\n",
    "    return list(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6191346739792809\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<491>198  85 |\n",
      "pos | 156<404> 64 |\n",
      "neg |  57  65<121>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.634      0.697      0.664\n",
      "neg \t      0.498      0.448      0.472\n",
      "pos \t      0.647      0.606      0.626\n",
      "1 0.6514320536258379\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<565>191  68 |\n",
      "pos | 133<404> 36 |\n",
      "neg |  76  68<100>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.686      0.730      0.707\n",
      "neg \t      0.410      0.490      0.446\n",
      "pos \t      0.705      0.609      0.654\n",
      "2 0.6179159049360147\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<543>185  82 |\n",
      "pos | 160<380> 41 |\n",
      "neg |  84  75 <91>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.670      0.690      0.680\n",
      "neg \t      0.364      0.425      0.392\n",
      "pos \t      0.654      0.594      0.622\n",
      "3 0.6179159049360147\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<504>186  61 |\n",
      "pos | 184<408> 44 |\n",
      "neg |  70  82<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.671      0.665      0.668\n",
      "neg \t      0.402      0.493      0.443\n",
      "pos \t      0.642      0.604      0.622\n",
      "4 0.6544789762340036\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<550>178  53 |\n",
      "pos | 169<441> 35 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.704      0.696      0.700\n",
      "neg \t      0.386      0.485      0.430\n",
      "pos \t      0.684      0.649      0.666\n",
      "mean accuracy 0.6321755027422303\n",
      "['damn', 'meant', 'kill', 'bless', 'breitbart', 'sb', 'suck', 'fuckin', 'losing', 'poetry', 'score', 'failed', 'report', 'changed', 'tired', 'window', 'except', 'kinda', 'six', 'swift', 'nooooooooo', 'h', 'favorite', 'enjoying', 'khl', 'body', 'bro', 'bored', 'sorry', 'funny', \"didn't\", 'deal', 'dwts', 'rafa', 'bullshit', 'kit', 'fucking', 'joke', 'channing', 'throw', 'plane', 'international', 'pavol', 'rookie', 'anymore', \"couldn't\", 'nice', 'awesome', 'mistake', 'bad', 'storm', 'compared', 'rider', 'interesting', 'factor', 'ticket', 'fit', 'cool', 'nothing', \"wouldn't\", 'option', 'die', 'hawaii', 'cliff', 'caltrain', 'brilliant', 'cuz', 'rohypnol', 'surprised', 'anarchy', 'sense', 'cry', 'leg', 'live', 'happened', 'shit', 'good', 'debate', 'emerson', 'missing', 'fucked', 'mad', 'fail', 'via', 'beating', 'cif', 'as', 'amazing', \"harry's\", \"can't\", 'service', 'gop', 'hell', 'outside', 'loss', 'stevie', 'believe', 'policy', 'potus', 'absolutely', 'attention', 'alone', 'wont', 'netanyahu', 'notebook', 'breakout', 'exciting', 'coz', 'afghan', 'fl', 'juice', 'bitch', 'dick', 'killed', 'matter', 'poll', 'doesnt', 'canceled', 'instead', 'mnf', 'fuck', 'worst', 'problem', 'dont', 'decided', 'hate', 'knew', 'injury', 'leaf', 'rejected', 'trayvon', 'avril', 'blame', 'forward', 'warned', 'smh', 'ceo', 'lb', 'luck', 'lovely', 'possibly', 'tatum', 'bench', 'bellamy', 'crap', 'excuse', 'horse', 'dead', 'johnny', 'delay', 'spirit', 'best', 'net', 'wait', 'fun', 'thank', 'computer', 'happy', 'serious', 'enjoy', 'sad', 'concert', 'drug', 'yay', 'dropping', 'scotland', 'evil', 'suppose', 'thanks', \"ain't\", 'jenelle', 'final', 'rudd', 'marijuana', 'parade', 'madonna', 'liked', 'hopefully', 'assist', 'andrew', 'wrong', \"hasn't\", 'award', 'protest', 'fear', 'penalty', 'boehner', 'perfect', 'error', 'cant', 'lucas', 'why', 'cancelled', 'voice', 'knocked', 'crash', 'fantastic', 'adnan', 'great', 'excited', 'twat', 'khan', 'trial', 'language', 'free', 'love', 'august', 'seem', 'dying', 'iebc', 'sick', 'demitra', 'weekend']\n"
     ]
    }
   ],
   "source": [
    "word_feature_final=cross_validation(5,featuresets)\n",
    "print(word_feature_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "['damn', 'meant', 'kill', 'bless', 'breitbart', 'sb', 'suck', 'fuckin', 'losing', 'poetry']\n",
      "['score', 'failed', 'report', 'changed', 'tired', 'window', 'except', 'kinda', 'six', 'swift']\n",
      "['nooooooooo', 'h', 'favorite', 'enjoying', 'khl', 'body', 'bro', 'bored', 'sorry', 'funny']\n"
     ]
    }
   ],
   "source": [
    "print(len(word_feature_final))\n",
    "print(word_feature_final[:10])\n",
    "print(word_feature_final[10:20])\n",
    "print(word_feature_final[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base line result is not good, I believe this is partly because of the limited data observations. Neutral tweets classification get better scores than positive ones, the negative ones have the worst scores.  \n",
    "  \n",
    "Among the 5 times of cross validations, each time we collected the most informative 100 words. However, there are only 213 unique words left. That means the tweets share many words that played a significant role in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram features generating and selection**  \n",
    "  \n",
    "1. Create bigram features using top 1000 best bigrams measuered by chi square\n",
    "2. Calculate baseline accuracy\n",
    "3. 10-fold cross validation, for each time, record the most informative 100 bigrams\n",
    "4. Gather the 10 100-bigram sets together as the useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 1000)\n",
    "\n",
    "def bigram_document_features(document, bigram_features):\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "0.47073170731707314\n"
     ]
    }
   ],
   "source": [
    "bigram_featuresets = [(bigram_document_features(d,bigram_features), c) for (d,c) in train_documents]\n",
    "thresh=int(len(bigram_featuresets)*0.1)\n",
    "print(thresh)\n",
    "train_set, test_set = bigram_featuresets[thresh:], bigram_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<386>  .   . |\n",
      "pos | 318  <.>  . |\n",
      "neg | 116   .  <.>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(classifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the bigrams won't give any valuable information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tagged features**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n",
    "\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6365853658536585"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=int(len(POS_featuresets)*0.1)\n",
    "train_set, test_set = POS_featuresets[thresh:], POS_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6288848263254113\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<503>184  87 |\n",
      "pos | 146<405> 73 |\n",
      "neg |  54  65<124>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.650      0.716      0.681\n",
      "neg \t      0.510      0.437      0.471\n",
      "pos \t      0.649      0.619      0.634\n",
      "1 0.6550883607556368\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<569>177  78 |\n",
      "pos | 135<401> 37 |\n",
      "neg |  74  65<105>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.691      0.731      0.710\n",
      "neg \t      0.430      0.477      0.453\n",
      "pos \t      0.700      0.624      0.660\n",
      "2 0.6227909811090798\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<548>177  85 |\n",
      "pos | 168<372> 41 |\n",
      "neg |  73  75<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.677      0.695      0.685\n",
      "neg \t      0.408      0.447      0.427\n",
      "pos \t      0.640      0.596      0.617\n",
      "3 0.6215722120658135\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<510>169  72 |\n",
      "pos | 186<404> 46 |\n",
      "neg |  69  79<106>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.679      0.667      0.673\n",
      "neg \t      0.417      0.473      0.444\n",
      "pos \t      0.635      0.620      0.627\n",
      "4 0.6514320536258379\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<556>168  57 |\n",
      "pos | 177<430> 38 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.712      0.692      0.702\n",
      "neg \t      0.386      0.466      0.422\n",
      "pos \t      0.667      0.653      0.660\n",
      "mean accuracy 0.6359536867763558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_feature_final=cross_validation(5,POS_featuresets)\n",
    "len(POS_feature_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def senti_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "\n",
    "    # record the polarity and subjectivity of each word\n",
    "    pol_list=[]\n",
    "    sub_list=[]\n",
    "    for word in document_words:\n",
    "        pol_list.append(TextBlob(word).polarity)\n",
    "        sub_list.append(TextBlob(word).subjectivity)\n",
    "    features['polarity']=sum(pol_list)/len(pol_list)\n",
    "    features['subjectivity']=sum(sub_list)/len(sub_list)\n",
    "    return features\n",
    "\n",
    "TBsenti_featuresets = [(senti_features(d, word_features), c) for (d, c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6707317073170732"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=int(len(TBsenti_featuresets)*0.1)\n",
    "train_set, test_set = TBsenti_featuresets[thresh:], TBsenti_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6569165143205362\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<551>150  73 |\n",
      "pos | 153<408> 63 |\n",
      "neg |  69  55<119>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.712      0.713      0.712\n",
      "neg \t      0.490      0.467      0.478\n",
      "pos \t      0.654      0.666      0.660\n",
      "1 0.680073126142596\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<612>149  63 |\n",
      "pos | 138<402> 33 |\n",
      "neg |  85  57<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.743      0.733      0.738\n",
      "neg \t      0.418      0.515      0.462\n",
      "pos \t      0.702      0.661      0.681\n",
      "2 0.6532602071907374\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<586>151  73 |\n",
      "pos | 155<386> 40 |\n",
      "neg |  83  67<100>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neu \t      0.723      0.711      0.717\n",
      "neg \t      0.400      0.469      0.432\n",
      "pos \t      0.664      0.639      0.651\n",
      "3 0.6422912858013407\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<547>140  64 |\n",
      "pos | 194<402> 40 |\n",
      "neg |  82  67<105>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg \t      0.413      0.502      0.454\n",
      "neu \t      0.728      0.665      0.695\n",
      "pos \t      0.632      0.660      0.646\n",
      "4 0.6837294332723949\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<601>134  46 |\n",
      "pos | 174<436> 35 |\n",
      "neg |  75  55 <85>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg \t      0.395      0.512      0.446\n",
      "neu \t      0.770      0.707      0.737\n",
      "pos \t      0.676      0.698      0.687\n",
      "mean accuracy 0.663254113345521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ntains(poll)',\n",
       " 'ntains(via)',\n",
       " 'ntains(favorite)',\n",
       " 'ntains(sick)',\n",
       " 'ntains(killed)',\n",
       " 'ntains(doesnt)',\n",
       " 'ntains(net)',\n",
       " 'ntains(hopefully)',\n",
       " 'ntains(serious)',\n",
       " \"ntains(hasn't)\",\n",
       " 'ntains(demitra)',\n",
       " 'ntains(missing)',\n",
       " 'ntains(score)',\n",
       " 'ntains(thanks)',\n",
       " 'ntains(emerson)',\n",
       " \"ntains(can't)\",\n",
       " 'ntains(sad)',\n",
       " 'ntains(twat)',\n",
       " 'ntains(fucked)',\n",
       " 'ntains(fit)',\n",
       " 'ntains(love)',\n",
       " 'ntains(rookie)',\n",
       " 'ntains(absolutely)',\n",
       " 'ntains(rudd)',\n",
       " 'ntains(scotland)',\n",
       " 'ntains(compared)',\n",
       " 'ntains(language)',\n",
       " 'ntains(knew)',\n",
       " 'ntains(injury)',\n",
       " 'ntains(bro)',\n",
       " 'ntains(body)',\n",
       " 'ntains(leg)',\n",
       " 'ntains(fl)',\n",
       " 'ntains(cancelled)',\n",
       " 'ntains(crap)',\n",
       " 'ntains(bellamy)',\n",
       " 'ntains(canceled)',\n",
       " 'ntains(fantastic)',\n",
       " 'ntains(trayvon)',\n",
       " 'larity',\n",
       " 'ntains(warned)',\n",
       " 'ntains(awesome)',\n",
       " 'ntains(weekend)',\n",
       " 'ntains(bitch)',\n",
       " 'ntains(funny)',\n",
       " 'ntains(anymore)',\n",
       " 'ntains(cuz)',\n",
       " 'ntains(leaf)',\n",
       " 'ntains(potus)',\n",
       " 'ntains(except)',\n",
       " 'ntains(nooooooooo)',\n",
       " 'ntains(wrong)',\n",
       " 'ntains(yay)',\n",
       " 'ntains(rafa)',\n",
       " 'ntains(matter)',\n",
       " 'ntains(wait)',\n",
       " 'ntains(award)',\n",
       " 'ntains(thank)',\n",
       " 'ntains(voice)',\n",
       " 'ntains(wont)',\n",
       " 'ntains(rider)',\n",
       " 'ntains(concert)',\n",
       " 'ntains(perfect)',\n",
       " 'ntains(meant)',\n",
       " 'ntains(as)',\n",
       " 'ntains(excuse)',\n",
       " 'ntains(service)',\n",
       " 'ntains(brilliant)',\n",
       " 'ntains(throw)',\n",
       " 'ntains(nice)',\n",
       " 'ntains(changed)',\n",
       " 'ntains(decided)',\n",
       " 'ntains(plane)',\n",
       " 'ntains(dwts)',\n",
       " 'ntains(why)',\n",
       " 'ntains(amazing)',\n",
       " 'ntains(poetry)',\n",
       " 'ntains(protest)',\n",
       " 'ntains(madonna)',\n",
       " 'ntains(excited)',\n",
       " 'ntains(fun)',\n",
       " 'ntains(fuckin)',\n",
       " 'ntains(caltrain)',\n",
       " 'ntains(damn)',\n",
       " 'ntains(surprised)',\n",
       " 'ntains(penalty)',\n",
       " 'ntains(free)',\n",
       " \"ntains(wouldn't)\",\n",
       " 'ntains(ceo)',\n",
       " 'ntains(failed)',\n",
       " 'ntains(worst)',\n",
       " 'ntains(bench)',\n",
       " 'ntains(boehner)',\n",
       " \"ntains(couldn't)\",\n",
       " 'ntains(interesting)',\n",
       " 'ntains(avril)',\n",
       " 'ntains(outside)',\n",
       " 'ntains(parade)',\n",
       " 'ntains(andrew)',\n",
       " 'ntains(enjoy)',\n",
       " 'ntains(exciting)',\n",
       " 'ntains(happy)',\n",
       " 'ntains(believe)',\n",
       " 'ntains(cant)',\n",
       " 'ntains(fear)',\n",
       " 'ntains(johnny)',\n",
       " 'ntains(shit)',\n",
       " 'ntains(final)',\n",
       " 'ntains(fail)',\n",
       " 'ntains(cry)',\n",
       " 'ntains(pavol)',\n",
       " 'ntains(six)',\n",
       " 'ntains(storm)',\n",
       " 'ntains(policy)',\n",
       " 'ntains(liked)',\n",
       " 'ntains(ticket)',\n",
       " 'ntains(breitbart)',\n",
       " 'ntains(factor)',\n",
       " 'ntains(horse)',\n",
       " 'ntains(great)',\n",
       " 'ntains(tired)',\n",
       " 'ntains(trial)',\n",
       " 'ntains(hawaii)',\n",
       " 'ntains(best)',\n",
       " 'bjectivity',\n",
       " 'ntains(suppose)',\n",
       " 'ntains(dying)',\n",
       " 'ntains(fucking)',\n",
       " 'ntains(kill)',\n",
       " 'ntains(crash)',\n",
       " 'ntains(sense)',\n",
       " 'ntains(happened)',\n",
       " 'ntains(kinda)',\n",
       " 'ntains(lucas)',\n",
       " 'ntains(evil)',\n",
       " 'ntains(losing)',\n",
       " 'ntains(beating)',\n",
       " 'ntains(juice)',\n",
       " 'ntains(instead)',\n",
       " 'ntains(deal)',\n",
       " 'ntains(joke)',\n",
       " 'ntains(smh)',\n",
       " 'ntains(seem)',\n",
       " 'ntains(report)',\n",
       " 'ntains(option)',\n",
       " 'ntains(debate)',\n",
       " 'ntains(sorry)',\n",
       " 'ntains(fuck)',\n",
       " 'ntains(good)',\n",
       " 'ntains(alone)',\n",
       " 'ntains(mistake)',\n",
       " 'ntains(bored)',\n",
       " 'ntains(anarchy)',\n",
       " 'ntains(iebc)',\n",
       " 'ntains(problem)',\n",
       " 'ntains(dead)',\n",
       " \"ntains(ain't)\",\n",
       " 'ntains(bad)',\n",
       " 'ntains(forward)',\n",
       " 'ntains(window)',\n",
       " 'ntains(suck)',\n",
       " \"ntains(harry's)\",\n",
       " 'ntains(dont)',\n",
       " 'ntains(computer)',\n",
       " 'ntains(breakout)',\n",
       " 'ntains(nothing)',\n",
       " 'ntains(bullshit)',\n",
       " 'ntains(afghan)',\n",
       " 'ntains(swift)',\n",
       " 'ntains(hell)',\n",
       " 'ntains(lb)',\n",
       " 'ntains(delay)',\n",
       " 'ntains(mad)',\n",
       " 'ntains(enjoying)',\n",
       " 'ntains(international)',\n",
       " 'ntains(spirit)',\n",
       " 'ntains(august)',\n",
       " 'ntains(lovely)',\n",
       " 'ntains(live)',\n",
       " 'ntains(assist)',\n",
       " 'ntains(rohypnol)',\n",
       " 'ntains(die)',\n",
       " 'ntains(loss)',\n",
       " 'ntains(bless)',\n",
       " 'ntains(h)',\n",
       " 'ntains(hate)',\n",
       " 'ntains(drug)',\n",
       " \"ntains(didn't)\",\n",
       " 'ntains(khl)',\n",
       " 'ntains(mnf)',\n",
       " 'ntains(cool)',\n",
       " 'ntains(channing)',\n",
       " 'ntains(luck)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(5,TBsenti_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion at this stage - Hang**  \n",
    "  \n",
    "For now, we did preprocessing on the data, tried classification with Naive Bayes model on the tokenized tweets using word-only, bigram, and POS-tagged features. We collected the most useful several hundred words for further classification tasks.  \n",
    "  \n",
    "We found:  \n",
    "1. Bigram features are found not valuable for classification\n",
    "2. POS-tagged features have slightly better result, compared with words-only ones.\n",
    "3. Apply other sentiment analysis API on a token level increased the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some plans for next steps - Hang**\n",
    "  \n",
    "1. Check the scores of no-stop-words version of the document (already made as \"train_document_nosw\")\n",
    "2. Try using some sentiment score APIs on the words so that we get sentiment values as features.\n",
    "3. Try other models for this classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
