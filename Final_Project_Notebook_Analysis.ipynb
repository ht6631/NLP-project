{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load & Preprocessing Function Description**  \n",
    "  \n",
    "1. Read tweets\n",
    "2. Extract text and sentiment label\n",
    "3. Tokenize, lower-case, lemmatize the tokens, then exclude punctuations, pure numbers and web links.\n",
    "4. Collect the processed tweeter text as a list of token list + sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Filters used in the main function below\n",
    "def filter_tokens(tokens):\n",
    "    pattern1 = re.compile(r'^(https?://[^\\s]+$|[^\\w\\s])')\n",
    "    pattern2 = re.compile(r'\\d+')\n",
    "    # Exclude punctuations and web links\n",
    "    filtered_ts = [token for token in tokens if not pattern1.match(token)]\n",
    "    filtered_ts= [token for token in filtered_ts if not pattern2.match(token)]\n",
    "    return filtered_ts\n",
    "def sw_filter(tokens):\n",
    "    # stopwords provided by 'TweetData'\n",
    "    stopwords = [line.strip() for line in open('TweetData/stopwords_twitter.txt')]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Main function\n",
    "def processtweets(path,nosw):\n",
    "\n",
    "    # initialize NLTK built-in tweet tokenizer\n",
    "    twtokenizer = TweetTokenizer()\n",
    "    # read file\n",
    "    f = open(path, 'r')\n",
    "\n",
    "    # gather the original data\n",
    "    tweetdata = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # each line has 4 items separated by tabs\n",
    "        # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "        tweetdata.append(line.split('\\t')[2:4])\n",
    "\n",
    "    # create list of tweet documents as (list of words, label)\n",
    "    # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "    # Create a list for the data\n",
    "    tweetdocs = []\n",
    "    # add all the tweets except the ones whose text is Not Available\n",
    "    neg_num=0\n",
    "    pos_num=0\n",
    "    neu_num=0\n",
    "    for tweet in tweetdata:\n",
    "        if (tweet[1] != 'Not Available'):\n",
    "            # tokenize each tweet text\n",
    "            tokens = twtokenizer.tokenize(tweet[1])\n",
    "            \n",
    "\n",
    "            # and used lemmatizer on them\n",
    "            tokens_lower=[token.lower() for token in tokens]\n",
    "            text=nltk.Text(tokens_lower)\n",
    "            wnl = nltk.WordNetLemmatizer()\n",
    "            tokens_lemma=[wnl.lemmatize(t) for t in text]\n",
    "            # Then filter out web pages and pure numbers and punctuations\n",
    "            tokens_filtered=filter_tokens(tokens_lemma)\n",
    "\n",
    "            # if we choose to exclude stop words\n",
    "            if nosw==1:\n",
    "                tokens_filtered=sw_filter(tokens_filtered)\n",
    "            \n",
    "            if tweet[0] == '\"negative\"':\n",
    "                label = 'neg'\n",
    "                neg_num+=1\n",
    "            elif tweet[0] == '\"positive\"':\n",
    "                label = 'pos'\n",
    "                pos_num+=1\n",
    "            else:\n",
    "                label='neu'\n",
    "                neu_num+=1\n",
    "\n",
    "            # add tokens, label to our document list\n",
    "            tweetdocs.append((tokens_filtered, label))\n",
    "    return [tweetdocs,pos_num,neg_num,neu_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3059, 1207, 3942]\n",
      "[491, 290, 632]\n",
      "8208\n",
      "1413\n",
      "['last', 'day', 'in', 'jeddah', 'will', 'be', 'in', 'brunei', 'tomorrow', 'night', 'and', 'then', 'surabaya', 'the', 'following', 'night', 'and', 'then', 'bali', 'the', 'night', 'after', 'that', 'whee']\n",
      "neu\n",
      "['last', 'day', 'jeddah', 'will', 'brunei', 'tomorrow', 'night', 'surabaya', 'following', 'night', 'bali', 'night', 'whee']\n",
      "neu\n"
     ]
    }
   ],
   "source": [
    "#In a.dev.dist.tsv are just ids, pure numbers of tweets, not useful.\n",
    "# b-dist can be seen as a train set while b.dev.dist is a separate test set we can use later.\n",
    "import random\n",
    "train_path='TweetData/corpus/downloaded-tweeti-b-dist.tsv'\n",
    "test_path='TweetData/corpus/downloaded-tweeti-b.dev.dist.tsv'\n",
    "\n",
    "# print number of tweets in each group\n",
    "train_documents,pos_num,neg_num,neu_num=processtweets(train_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "test_documents,pos_num,neg_num,neu_num=processtweets(test_path,0)\n",
    "print([pos_num,neg_num,neu_num])\n",
    "\n",
    "train_documents_nosw=processtweets(train_path,1)[0]\n",
    "test_documents_nosw=processtweets(test_path,1)[0]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents)\n",
    "random.seed(42)\n",
    "random.shuffle(train_documents_nosw)\n",
    "\n",
    "# Print the amount of tweets\n",
    "print(len(train_documents))\n",
    "print(len(test_documents))\n",
    "\n",
    "# Print the tokenized tweet and label of the thrid document\n",
    "print(train_documents[2][0])\n",
    "print(train_documents[2][1])\n",
    "print(train_documents_nosw[2][0])\n",
    "print(train_documents_nosw[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral tweets are the most, followed by positive ones and negative ones are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word features generating and selection**  \n",
    "  \n",
    "1. Create word features using top half of frequent words\n",
    "2. Calculate baseline accuracy\n",
    "3. 5-fold cross validation, for each time, record the most informative 100 words\n",
    "4. Gather the 10 100-words sets together as the most useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14069 unique words in total\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in train_documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print('There are',len(all_words),'unique words in total')\n",
    "word_items = all_words.most_common(int(0.5*len(all_words)))\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_{}'.format(word)] = (word in document_words)\n",
    "\treturn features\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n"
     ]
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "train_set, test_set = featuresets[800:], featuresets[:800]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cross validation function, after regular printing accuracy, confusion matrix, precision, recall and F-1 scores, the most informative 100 word features each round are collected as a python set. \n",
    "  \n",
    "We will use those word features for further classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # confusion matrix\n",
    "    cm = nltk.ConfusionMatrix(gold, predicted)\n",
    "    print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))\n",
    "\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    word_set=set()\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:(i+1)*subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "        # train using train_this_round\n",
    "        classifier_this_round = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        infeatures=classifier_this_round.most_informative_features(100)\n",
    "        for item in infeatures:\n",
    "            word=item[0][2:]\n",
    "            word_set.add(word)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier_this_round, test_this_round)\n",
    "        print(i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "\n",
    "        # predicted and test \n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier_this_round.classify(features))\n",
    "\n",
    "        # print confusion matrix and evaluating measures\n",
    "        eval_measures(goldlist,predictedlist)\n",
    "\n",
    "    \n",
    "    # find mean accuracy over all rounds\n",
    "    print('mean accuracy', sum(accuracy_list) / num_folds)\n",
    "    return list(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6197440585009141\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<492>198  84 |\n",
      "pos | 155<405> 64 |\n",
      "neg |  58  65<120>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.649      0.606      0.627\n",
      "neg \t      0.494      0.448      0.470\n",
      "neu \t      0.636      0.698      0.665\n",
      "1 0.6520414381474711\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<565>191  68 |\n",
      "pos | 133<405> 35 |\n",
      "neg |  76  68<100>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.707      0.610      0.655\n",
      "neg \t      0.410      0.493      0.447\n",
      "neu \t      0.686      0.730      0.707\n",
      "2 0.6185252894576477\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<543>185  82 |\n",
      "pos | 160<380> 41 |\n",
      "neg |  83  75 <92>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.654      0.594      0.622\n",
      "neg \t      0.368      0.428      0.396\n",
      "neu \t      0.670      0.691      0.680\n",
      "3 0.6203534430225472\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<506>185  60 |\n",
      "pos | 184<408> 44 |\n",
      "neg |  69  81<104>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.642      0.605      0.623\n",
      "neg \t      0.409      0.500      0.450\n",
      "neu \t      0.674      0.667      0.670\n",
      "4 0.6532602071907374\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<549>178  54 |\n",
      "pos | 169<440> 36 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.682      0.648      0.665\n",
      "neg \t      0.386      0.480      0.428\n",
      "neu \t      0.703      0.696      0.699\n",
      "mean accuracy 0.6327848872638635\n",
      "['worst', 'coz', 'ceo', 'why', 'potus', 'loss', 'bitch', 'dropping', 'suppose', 'cool', 'h', 'love', 'marijuana', 'luck', 'option', 'rookie', 'fl', 'suck', 'as', 'error', 'fucked', 'tired', 'delay', 'less', 'netanyahu', 'killed', 'fuckin', 'possibly', 'mistake', 'stevie', \"can't\", 'liked', 'amazing', 'window', 'lovely', 'hawaii', 'breakout', 'decided', 'final', 'good', 'forward', 'canceled', 'nooooooooo', 'trayvon', 'alone', 'favorite', 'free', 'hopefully', 'serious', 'avril', 'horse', 'drug', 'madonna', 'kinda', 'beating', 'plane', 'kill', 'failed', 'warned', 'crash', 'injury', 'fantastic', 'breitbart', 'sb', 'problem', 'happened', 'gop', 'throw', 'fail', 'juice', 'wait', 'assist', 'spirit', 'bro', 'iebc', 'believe', 'great', 'thank', 'service', 'fun', 'doesnt', 'fit', 'poll', 'rudd', 'voice', 'losing', 'dick', 'best', 'lucas', 'afghan', \"ain't\", 'score', 'leg', 'rafa', 'knew', 'caltrain', 'cancelled', 'joke', 'outside', 'blame', 'storm', 'emerson', 'mnf', 'shit', 'die', 'cant', 'cry', 'fuck', 'evil', 'adnan', 'fucking', 'dont', 'via', 'cuz', 'dying', 'body', 'deal', 'enjoy', 'channing', 'computer', 'funny', 'policy', \"couldn't\", 'nice', 'missing', 'mad', 'sense', 'concert', 'penalty', \"harry's\", 'seem', 'language', 'hell', 'sad', \"hasn't\", 'twat', 'excuse', 'trial', 'smh', 'debate', 'rejected', 'weekend', 'yay', 'andrew', 'pavol', 'bench', 'compared', 'leaf', 'report', 'anymore', 'august', 'sorry', 'rohypnol', 'notebook', 'live', 'jenelle', 'net', 'lb', 'meant', 'exciting', 'interesting', 'factor', 'khl', 'hate', 'dead', 'knocked', 'thanks', 'instead', 'wrong', 'boehner', 'parade', 'kit', 'fear', 'enjoying', 'johnny', 'bad', 'attention', 'bellamy', 'matter', 'cif', 'changed', 'happy', 'rider', 'crap', 'ticket', 'award', 'demitra', 'brilliant', 'bored', 'poetry', 'excited', \"wouldn't\", \"didn't\", 'nothing', 'surprised', 'damn', 'except', 'khan', 'international', 'protest', 'wont', 'scotland', 'awesome', 'bless', 'six', 'anarchy', 'dwts', 'perfect', 'cliff', 'absolutely', 'sick', 'bullshit', 'swift']\n"
     ]
    }
   ],
   "source": [
    "word_feature_final=cross_validation(5,featuresets)\n",
    "print(word_feature_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "['worst', 'coz', 'ceo', 'why', 'potus', 'loss', 'bitch', 'dropping', 'suppose', 'cool']\n",
      "['h', 'love', 'marijuana', 'luck', 'option', 'rookie', 'fl', 'suck', 'as', 'error']\n",
      "['fucked', 'tired', 'delay', 'less', 'netanyahu', 'killed', 'fuckin', 'possibly', 'mistake', 'stevie']\n"
     ]
    }
   ],
   "source": [
    "print(len(word_feature_final))\n",
    "print(word_feature_final[:10])\n",
    "print(word_feature_final[10:20])\n",
    "print(word_feature_final[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base line result is not good, I believe this is partly because of the limited data observations. Neutral tweets classification get better scores than positive ones, the negative ones have the worst scores.  \n",
    "  \n",
    "Among the 5 times of cross validations, each time we collected the most informative 100 words. However, there are only 213 unique words left. That means the tweets share many words that played a significant role in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram features generating and selection**  \n",
    "  \n",
    "1. Create bigram features using top 1000 best bigrams measuered by chi square\n",
    "2. Calculate baseline accuracy\n",
    "3. 10-fold cross validation, for each time, record the most informative 100 bigrams\n",
    "4. Gather the 10 100-bigram sets together as the useful features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 1000)\n",
    "\n",
    "def bigram_document_features(document, bigram_features):\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "0.47073170731707314\n"
     ]
    }
   ],
   "source": [
    "bigram_featuresets = [(bigram_document_features(d,bigram_features), c) for (d,c) in train_documents]\n",
    "thresh=int(len(bigram_featuresets)*0.1)\n",
    "print(thresh)\n",
    "train_set, test_set = bigram_featuresets[thresh:], bigram_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<386>  .   . |\n",
      "pos | 318  <.>  . |\n",
      "neg | 116   .  <.>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(classifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the bigrams won't give any valuable information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tagged features**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n",
    "\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6365853658536585"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=int(len(POS_featuresets)*0.1)\n",
    "train_set, test_set = POS_featuresets[thresh:], POS_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6301035953686777\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<504>183  87 |\n",
      "pos | 145<406> 73 |\n",
      "neg |  54  65<124>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.651      0.621      0.635\n",
      "neg \t      0.510      0.437      0.471\n",
      "neu \t      0.651      0.717      0.682\n",
      "1 0.6544789762340036\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<566>179  79 |\n",
      "pos | 133<403> 37 |\n",
      "neg |  74  65<105>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.703      0.623      0.661\n",
      "neg \t      0.430      0.475      0.452\n",
      "neu \t      0.687      0.732      0.709\n",
      "2 0.6240097501523462\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<549>177  84 |\n",
      "pos | 168<372> 41 |\n",
      "neg |  72  75<103>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.640      0.596      0.617\n",
      "neg \t      0.412      0.452      0.431\n",
      "neu \t      0.678      0.696      0.687\n",
      "3 0.6227909811090798\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<512>167  72 |\n",
      "pos | 186<404> 46 |\n",
      "neg |  69  79<106>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.635      0.622      0.628\n",
      "neg \t      0.417      0.473      0.444\n",
      "neu \t      0.682      0.668      0.675\n",
      "4 0.6502132845825715\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<555>168  58 |\n",
      "pos | 177<429> 39 |\n",
      "neg |  71  61 <83>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.665      0.652      0.658\n",
      "neg \t      0.386      0.461      0.420\n",
      "neu \t      0.711      0.691      0.701\n",
      "mean accuracy 0.6363193174893358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_feature_final=cross_validation(5,POS_featuresets)\n",
    "len(POS_feature_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polarity features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def senti_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "\n",
    "    # record the polarity and subjectivity of each word\n",
    "    pol_list=[]\n",
    "    sub_list=[]\n",
    "    for word in document_words:\n",
    "        pol_list.append(TextBlob(word).polarity)\n",
    "        sub_list.append(TextBlob(word).subjectivity)\n",
    "    features['polarity']=sum(pol_list)/len(pol_list)\n",
    "    features['subjectivity']=sum(sub_list)/len(sub_list)\n",
    "    return features\n",
    "\n",
    "TBsenti_featuresets = [(senti_features(d, word_features), c) for (d, c) in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6743902439024391"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=int(len(TBsenti_featuresets)*0.1)\n",
    "train_set, test_set = TBsenti_featuresets[thresh:], TBsenti_featuresets[:thresh]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6532602071907374\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<549>150  75 |\n",
      "pos | 154<404> 66 |\n",
      "neg |  69  55<119>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.647      0.663      0.655\n",
      "neg \t      0.490      0.458      0.473\n",
      "neu \t      0.709      0.711      0.710\n",
      "1 0.6794637416209628\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<612>149  63 |\n",
      "pos | 139<401> 33 |\n",
      "neg |  83  59<102>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.700      0.658      0.679\n",
      "neg \t      0.418      0.515      0.462\n",
      "neu \t      0.743      0.734      0.738\n",
      "2 0.6544789762340036\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<588>150  72 |\n",
      "pos | 155<386> 40 |\n",
      "neg |  84  66<100>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.664      0.641      0.653\n",
      "neg \t      0.400      0.472      0.433\n",
      "neu \t      0.726      0.711      0.718\n",
      "3 0.643510054844607\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<549>140  62 |\n",
      "pos | 194<402> 40 |\n",
      "neg |  83  66<105>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.632      0.661      0.646\n",
      "neg \t      0.413      0.507      0.456\n",
      "neu \t      0.731      0.665      0.696\n",
      "4 0.6837294332723949\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<599>135  47 |\n",
      "pos | 173<438> 34 |\n",
      "neg |  76  54 <85>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.679      0.699      0.689\n",
      "neg \t      0.395      0.512      0.446\n",
      "neu \t      0.767      0.706      0.735\n",
      "mean accuracy 0.6628884826325412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ntains(dying)',\n",
       " 'ntains(computer)',\n",
       " 'ntains(parade)',\n",
       " \"ntains(hasn't)\",\n",
       " 'ntains(hate)',\n",
       " 'ntains(service)',\n",
       " 'ntains(final)',\n",
       " 'ntains(fuckin)',\n",
       " 'ntains(smh)',\n",
       " 'ntains(dont)',\n",
       " 'ntains(report)',\n",
       " 'ntains(demitra)',\n",
       " 'ntains(damn)',\n",
       " 'ntains(except)',\n",
       " 'ntains(khl)',\n",
       " 'ntains(language)',\n",
       " 'ntains(changed)',\n",
       " 'ntains(ceo)',\n",
       " 'ntains(crap)',\n",
       " 'ntains(juice)',\n",
       " 'ntains(hell)',\n",
       " 'ntains(enjoy)',\n",
       " 'ntains(kinda)',\n",
       " \"ntains(ain't)\",\n",
       " 'ntains(bro)',\n",
       " 'ntains(international)',\n",
       " 'ntains(trial)',\n",
       " 'ntains(andrew)',\n",
       " 'ntains(policy)',\n",
       " 'ntains(penalty)',\n",
       " 'ntains(luck)',\n",
       " 'ntains(caltrain)',\n",
       " 'ntains(awesome)',\n",
       " 'ntains(fl)',\n",
       " 'ntains(believe)',\n",
       " 'ntains(absolutely)',\n",
       " 'ntains(factor)',\n",
       " 'ntains(less)',\n",
       " 'ntains(killed)',\n",
       " 'ntains(compared)',\n",
       " 'ntains(pavol)',\n",
       " \"ntains(wouldn't)\",\n",
       " 'ntains(seem)',\n",
       " 'ntains(best)',\n",
       " 'ntains(enjoying)',\n",
       " 'ntains(poll)',\n",
       " 'ntains(dwts)',\n",
       " 'ntains(debate)',\n",
       " \"ntains(couldn't)\",\n",
       " 'ntains(deal)',\n",
       " 'ntains(love)',\n",
       " 'ntains(alone)',\n",
       " 'ntains(sick)',\n",
       " 'ntains(score)',\n",
       " 'ntains(thanks)',\n",
       " 'ntains(liked)',\n",
       " 'ntains(good)',\n",
       " 'ntains(suppose)',\n",
       " 'ntains(six)',\n",
       " 'ntains(dead)',\n",
       " 'ntains(doesnt)',\n",
       " 'ntains(why)',\n",
       " 'ntains(warned)',\n",
       " 'ntains(cant)',\n",
       " 'ntains(worst)',\n",
       " 'ntains(free)',\n",
       " 'ntains(yay)',\n",
       " 'ntains(favorite)',\n",
       " 'ntains(leg)',\n",
       " 'ntains(h)',\n",
       " 'ntains(fun)',\n",
       " 'ntains(hopefully)',\n",
       " 'ntains(avril)',\n",
       " 'ntains(happened)',\n",
       " 'ntains(beating)',\n",
       " 'ntains(wont)',\n",
       " 'ntains(tired)',\n",
       " 'ntains(hawaii)',\n",
       " 'ntains(fail)',\n",
       " 'ntains(swift)',\n",
       " 'ntains(storm)',\n",
       " 'ntains(nice)',\n",
       " 'ntains(loss)',\n",
       " 'ntains(spirit)',\n",
       " 'ntains(wrong)',\n",
       " 'ntains(sorry)',\n",
       " 'ntains(breitbart)',\n",
       " 'ntains(cool)',\n",
       " 'ntains(assist)',\n",
       " 'ntains(twat)',\n",
       " 'ntains(missing)',\n",
       " 'ntains(fuck)',\n",
       " 'ntains(nothing)',\n",
       " 'ntains(rookie)',\n",
       " 'ntains(lb)',\n",
       " 'ntains(bullshit)',\n",
       " 'ntains(cuz)',\n",
       " 'ntains(voice)',\n",
       " 'ntains(lucas)',\n",
       " 'ntains(cry)',\n",
       " 'ntains(poetry)',\n",
       " 'ntains(mad)',\n",
       " 'ntains(perfect)',\n",
       " 'ntains(throw)',\n",
       " 'ntains(knew)',\n",
       " 'ntains(cancelled)',\n",
       " 'ntains(via)',\n",
       " 'ntains(interesting)',\n",
       " 'ntains(anymore)',\n",
       " 'ntains(iebc)',\n",
       " 'ntains(fit)',\n",
       " 'ntains(leaf)',\n",
       " 'ntains(boehner)',\n",
       " 'ntains(nooooooooo)',\n",
       " 'bjectivity',\n",
       " 'ntains(joke)',\n",
       " 'ntains(rafa)',\n",
       " 'ntains(instead)',\n",
       " 'ntains(fucked)',\n",
       " 'ntains(body)',\n",
       " 'ntains(potus)',\n",
       " 'ntains(lovely)',\n",
       " 'ntains(thank)',\n",
       " 'ntains(problem)',\n",
       " 'ntains(august)',\n",
       " 'ntains(window)',\n",
       " 'ntains(afghan)',\n",
       " 'ntains(scotland)',\n",
       " 'ntains(failed)',\n",
       " 'ntains(great)',\n",
       " 'ntains(bored)',\n",
       " 'ntains(option)',\n",
       " 'ntains(die)',\n",
       " 'ntains(live)',\n",
       " 'ntains(sad)',\n",
       " 'ntains(fear)',\n",
       " 'ntains(channing)',\n",
       " 'ntains(concert)',\n",
       " 'ntains(delay)',\n",
       " 'ntains(bless)',\n",
       " 'ntains(shit)',\n",
       " 'ntains(anarchy)',\n",
       " 'ntains(excited)',\n",
       " 'ntains(trayvon)',\n",
       " 'ntains(protest)',\n",
       " 'ntains(suck)',\n",
       " 'ntains(forward)',\n",
       " 'ntains(fantastic)',\n",
       " 'ntains(award)',\n",
       " \"ntains(harry's)\",\n",
       " 'ntains(sense)',\n",
       " 'ntains(outside)',\n",
       " 'ntains(crash)',\n",
       " 'larity',\n",
       " 'ntains(horse)',\n",
       " \"ntains(can't)\",\n",
       " 'ntains(amazing)',\n",
       " 'ntains(as)',\n",
       " 'ntains(bad)',\n",
       " 'ntains(plane)',\n",
       " 'ntains(exciting)',\n",
       " 'ntains(emerson)',\n",
       " 'ntains(bellamy)',\n",
       " 'ntains(decided)',\n",
       " 'ntains(funny)',\n",
       " 'ntains(canceled)',\n",
       " 'ntains(mistake)',\n",
       " 'ntains(matter)',\n",
       " 'ntains(rudd)',\n",
       " 'ntains(breakout)',\n",
       " 'ntains(serious)',\n",
       " 'ntains(happy)',\n",
       " 'ntains(ticket)',\n",
       " 'ntains(bitch)',\n",
       " 'ntains(surprised)',\n",
       " 'ntains(rider)',\n",
       " 'ntains(drug)',\n",
       " 'ntains(wait)',\n",
       " 'ntains(kill)',\n",
       " 'ntains(excuse)',\n",
       " 'ntains(losing)',\n",
       " \"ntains(didn't)\",\n",
       " 'ntains(evil)',\n",
       " 'ntains(mnf)',\n",
       " 'ntains(johnny)',\n",
       " 'ntains(injury)',\n",
       " 'ntains(rohypnol)',\n",
       " 'ntains(net)',\n",
       " 'ntains(weekend)',\n",
       " 'ntains(fucking)',\n",
       " 'ntains(madonna)',\n",
       " 'ntains(brilliant)',\n",
       " 'ntains(bench)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(5,TBsenti_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion at this stage - Hang**  \n",
    "  \n",
    "For now, we did preprocessing on the data, tried classification with Naive Bayes model on the tokenized tweets using word-only, bigram, and POS-tagged features. We collected the most useful several hundred words for further classification tasks.  \n",
    "  \n",
    "We found:  \n",
    "1. Bigram features are found not valuable for classification\n",
    "2. POS-tagged features have slightly better result, compared with words-only ones.\n",
    "3. Apply other sentiment analysis API on a token level increased the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some plans for Future next steps - Hang**\n",
    "  \n",
    "1. Check the scores of no-stop-words version of the document (already made as \"train_document_nosw\")\n",
    "2. Try using some sentiment score APIs on the words so that we get sentiment values as features.\n",
    "3. Try other models for this classification problem.\n",
    "4. Creating more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As per advice from my teamate Hang, Im working on trying more algorithms and Creating new features, and trying them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional changes - Vaishnavi Meka Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - I'm trying experiments which includes trying bi-grams, tri-grams, with pos features and at last combining all features with Different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature generation code\n",
    "def bigram_pos_features(document):\n",
    "    \"\"\"Extract bigram + POS features.\"\"\"\n",
    "    document_bigrams = list(nltk.bigrams(document))\n",
    "    tagged_bigrams = nltk.pos_tag([word for bigram in document_bigrams for word in bigram])\n",
    "    \n",
    "    features = {}\n",
    "    for i, (word, tag) in enumerate(tagged_bigrams):\n",
    "        features[f'B_{word}_POS_{tag}'] = True\n",
    "    return features\n",
    "\n",
    "def trigram_pos_features(document):\n",
    "    \"\"\"Extract trigram + POS features.\"\"\"\n",
    "    document_trigrams = list(nltk.trigrams(document))\n",
    "    tagged_trigrams = nltk.pos_tag([word for trigram in document_trigrams for word in trigram])\n",
    "    \n",
    "    features = {}\n",
    "    for i, (word, tag) in enumerate(tagged_trigrams):\n",
    "        features[f'T_{word}_POS_{tag}'] = True\n",
    "    return features\n",
    "\n",
    "# Combine all features\n",
    "def combined_features(document, word_features, bigram_features, trigram_features):\n",
    "    \"\"\"Combine word, bigram, trigram, and POS features.\"\"\"\n",
    "    document_words = set(document)\n",
    "    document_bigrams = list(nltk.bigrams(document))\n",
    "    document_trigrams = list(nltk.trigrams(document))\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "\n",
    "    features = {}\n",
    "    # Word-level features\n",
    "    for word in word_features:\n",
    "        features[f'W_{word}'] = (word in document_words)\n",
    "    \n",
    "    # Bigram-level features\n",
    "    for bigram in bigram_features:\n",
    "        features[f'B_{bigram[0]}_{bigram[1]}'] = (bigram in document_bigrams)\n",
    "    \n",
    "    # Trigram-level features\n",
    "    for trigram in trigram_features:\n",
    "        features[f'T_{trigram[0]}_{trigram[1]}_{trigram[2]}'] = (trigram in document_trigrams)\n",
    "    \n",
    "    # POS-level features\n",
    "    num_nouns = sum(1 for word, tag in tagged_words if tag.startswith('N'))\n",
    "    num_verbs = sum(1 for word, tag in tagged_words if tag.startswith('V'))\n",
    "    num_adjs = sum(1 for word, tag in tagged_words if tag.startswith('J'))\n",
    "    num_advs = sum(1 for word, tag in tagged_words if tag.startswith('R'))\n",
    "\n",
    "    features['num_nouns'] = num_nouns\n",
    "    features['num_verbs'] = num_verbs\n",
    "    features['num_adjectives'] = num_adjs\n",
    "    features['num_adverbs'] = num_advs\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Generate combined feature sets\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 1000)\n",
    "trigram_features = list(nltk.trigrams(all_words_list))[:1000]  # Top 1000 trigrams\n",
    "\n",
    "combined_featuresets = [\n",
    "    (combined_features(d, word_features, bigram_features, trigram_features), c)\n",
    "    for (d, c) in train_documents\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining NB Classifier from NLTK Class\n",
    "\n",
    "# Function to evaluate a feature set\n",
    "def evaluate_features(featuresets, description):\n",
    "    \"\"\"Train and evaluate a classifier on the given feature sets.\"\"\"\n",
    "    thresh = int(len(featuresets) * 0.1)  # Use 10% for testing\n",
    "    train_set, test_set = featuresets[thresh:], featuresets[:thresh]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    print(f\"Accuracy with {description}: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    goldlist = []\n",
    "    predictedlist = []\n",
    "    for (features, label) in test_set:\n",
    "        goldlist.append(label)\n",
    "        predictedlist.append(classifier.classify(features))\n",
    "    cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "    print(f\"Confusion Matrix for {description}:\\n{cm.pretty_format(sort_by_count=True, show_percents=False, truncate=9)}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining SVC Classifier from Sklearn class\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_features_svm(featuresets, description):\n",
    "    \"\"\"Train and evaluate an SVM classifier on the given feature sets.\"\"\"\n",
    "    thresh = int(len(featuresets) * 0.1)  # Use 10% for testing\n",
    "    train_set, test_set = featuresets[thresh:], featuresets[:thresh]\n",
    "\n",
    "    # Separate features and labels for training and testing\n",
    "    train_X = [features for features, label in train_set]\n",
    "    train_y = [label for features, label in train_set]\n",
    "    test_X = [features for features, label in test_set]\n",
    "    test_y = [label for features, label in test_set]\n",
    "\n",
    "    # Convert feature dictionaries into feature matrices\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    train_X = vectorizer.fit_transform(train_X)\n",
    "    test_X = vectorizer.transform(test_X)\n",
    "\n",
    "    # Train SVM classifier\n",
    "    svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "    svm_classifier.fit(train_X, train_y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predicted_y = svm_classifier.predict(test_X)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(test_y, predicted_y)\n",
    "    print(f\"Accuracy with {description}: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_y, predicted_y)\n",
    "    print(f\"Confusion Matrix for {description}:\\n{cm}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_y, predicted_y))\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. performing evaluation using NB model from NLTK Class on Bigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bigram + POS features: 0.5402\n",
      "Confusion Matrix for Bigram + POS features:\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<160> 95 131 |\n",
      "pos |  34<190> 94 |\n",
      "neg |   8  15 <93>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5402439024390244"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram + POS Features Evluating using NB Classifier\n",
    "bigram_pos_featuresets = [\n",
    "    (bigram_pos_features(d), c) for (d, c) in train_documents\n",
    "]\n",
    "evaluate_features(bigram_pos_featuresets, \"Bigram + POS features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. performing evaluation using NB model from NLTK Class on Trigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Trigram + POS features: 0.5305\n",
      "Confusion Matrix for Trigram + POS features:\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<148> 84 154 |\n",
      "pos |  35<190> 93 |\n",
      "neg |   8  11 <97>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5304878048780488"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigram + POS Features Evluating using NB Classifier\n",
    "trigram_pos_featuresets = [\n",
    "    (trigram_pos_features(d), c) for (d, c) in train_documents\n",
    "]\n",
    "evaluate_features(trigram_pos_featuresets, \"Trigram + POS features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. performing evaluation using NB model from NLTK Class on  Bigram features + Trigram + word features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Combined features: 0.6024\n",
      "Confusion Matrix for Combined features:\n",
      "    |   n   p   n |\n",
      "    |   e   o   e |\n",
      "    |   u   s   g |\n",
      "----+-------------+\n",
      "neu |<221> 88  77 |\n",
      "pos |  64<205> 49 |\n",
      "neg |  18  30 <68>|\n",
      "----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6024390243902439"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combined Features (for comparison) using NB Classifier\n",
    "combined_featuresets = [\n",
    "    (combined_features(d, word_features, bigram_features, trigram_features), c)\n",
    "    for (d, c) in train_documents\n",
    "]\n",
    "evaluate_features(combined_featuresets, \"Combined features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Performing evaluation using SVM model from Sklearn Class on Bigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying SVM to check if we can improve any accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bigram + POS features: 0.6256\n",
      "Confusion Matrix for Bigram + POS features:\n",
      "[[ 52  49  15]\n",
      " [ 32 274  80]\n",
      " [ 37  94 187]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.43      0.45      0.44       116\n",
      "         neu       0.66      0.71      0.68       386\n",
      "         pos       0.66      0.59      0.62       318\n",
      "\n",
      "    accuracy                           0.63       820\n",
      "   macro avg       0.58      0.58      0.58       820\n",
      "weighted avg       0.63      0.63      0.63       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625609756097561"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_svm(bigram_pos_featuresets, \"Bigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Performing evaluation using SVM model from Sklearn Class on Trigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Trigram + POS features: 0.6329\n",
      "Confusion Matrix for Trigram + POS features:\n",
      "[[ 57  41  18]\n",
      " [ 36 272  78]\n",
      " [ 28 100 190]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.47      0.49      0.48       116\n",
      "         neu       0.66      0.70      0.68       386\n",
      "         pos       0.66      0.60      0.63       318\n",
      "\n",
      "    accuracy                           0.63       820\n",
      "   macro avg       0.60      0.60      0.60       820\n",
      "weighted avg       0.63      0.63      0.63       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6329268292682927"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_svm(trigram_pos_featuresets, \"Trigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Performing evaluation using SVM model from Sklearn Class on Bigram features + word Features + trigram features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Combined features: 0.6366\n",
      "Confusion Matrix for Combined features:\n",
      "[[ 53  44  19]\n",
      " [ 41 267  78]\n",
      " [ 35  81 202]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.41      0.46      0.43       116\n",
      "         neu       0.68      0.69      0.69       386\n",
      "         pos       0.68      0.64      0.65       318\n",
      "\n",
      "    accuracy                           0.64       820\n",
      "   macro avg       0.59      0.59      0.59       820\n",
      "weighted avg       0.64      0.64      0.64       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6365853658536585"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_svm(combined_featuresets, \"Combined features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying Random Forest to check if we can improve any accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_features_rf(featuresets, description, n_estimators=100):\n",
    "    \"\"\"Train and evaluate a Random Forest classifier on the given feature sets.\"\"\"\n",
    "    thresh = int(len(featuresets) * 0.1)  # Use 10% for testing\n",
    "    train_set, test_set = featuresets[thresh:], featuresets[:thresh]\n",
    "\n",
    "    # Separate features and labels for training and testing\n",
    "    train_X = [features for features, label in train_set]\n",
    "    train_y = [label for features, label in train_set]\n",
    "    test_X = [features for features, label in test_set]\n",
    "    test_y = [label for features, label in test_set]\n",
    "\n",
    "    # Convert feature dictionaries into feature matrices\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    train_X = vectorizer.fit_transform(train_X)\n",
    "    test_X = vectorizer.transform(test_X)\n",
    "\n",
    "    # Train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_classifier.fit(train_X, train_y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predicted_y = rf_classifier.predict(test_X)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(test_y, predicted_y)\n",
    "    print(f\"Accuracy with {description}: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_y, predicted_y)\n",
    "    print(f\"Confusion Matrix for {description}:\\n{cm}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_y, predicted_y))\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Performing evaluation using Random Forest model from Sklearn Class on Bigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bigram + POS features: 0.6317\n",
      "Confusion Matrix for Bigram + POS features:\n",
      "[[  7  87  22]\n",
      " [  4 348  34]\n",
      " [  2 153 163]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.06      0.11       116\n",
      "         neu       0.59      0.90      0.71       386\n",
      "         pos       0.74      0.51      0.61       318\n",
      "\n",
      "    accuracy                           0.63       820\n",
      "   macro avg       0.62      0.49      0.48       820\n",
      "weighted avg       0.64      0.63      0.59       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6317073170731707"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_rf(bigram_pos_featuresets, \"Bigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Performing evaluation using Random Forest model from Sklearn Class on Trigram features + Pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Trigram + POS features: 0.6402\n",
      "Confusion Matrix for Trigram + POS features:\n",
      "[[  7  90  19]\n",
      " [  3 350  33]\n",
      " [  3 147 168]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.06      0.11       116\n",
      "         neu       0.60      0.91      0.72       386\n",
      "         pos       0.76      0.53      0.62       318\n",
      "\n",
      "    accuracy                           0.64       820\n",
      "   macro avg       0.63      0.50      0.48       820\n",
      "weighted avg       0.65      0.64      0.60       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6402439024390244"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_rf(trigram_pos_featuresets, \"Trigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Performing evaluation using Random Forest model from Sklearn Class on Bigram features + word + trigram Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Combined features: 0.6366\n",
      "Confusion Matrix for Combined features:\n",
      "[[ 10  87  19]\n",
      " [  3 348  35]\n",
      " [  2 152 164]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.09      0.15       116\n",
      "         neu       0.59      0.90      0.72       386\n",
      "         pos       0.75      0.52      0.61       318\n",
      "\n",
      "    accuracy                           0.64       820\n",
      "   macro avg       0.67      0.50      0.49       820\n",
      "weighted avg       0.67      0.64      0.60       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6365853658536585"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_features_rf(combined_featuresets, \"Combined features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do better ? trying ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_ensemble(featuresets, description):\n",
    "    \"\"\"Train and evaluate an ensemble model on the given feature sets.\"\"\"\n",
    "    # Split into train and test sets\n",
    "    thresh = int(len(featuresets) * 0.1)  # Use 10% for testing\n",
    "    train_set, test_set = featuresets[thresh:], featuresets[:thresh]\n",
    "\n",
    "    # Separate features and labels\n",
    "    train_X = [features for features, label in train_set]\n",
    "    train_y = [label for features, label in train_set]\n",
    "    test_X = [features for features, label in test_set]\n",
    "    test_y = [label for features, label in test_set]\n",
    "\n",
    "    # Convert feature dictionaries to feature matrices\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    train_X = vectorizer.fit_transform(train_X)\n",
    "    test_X = vectorizer.transform(test_X)\n",
    "\n",
    "    # Initialize individual classifiers\n",
    "    svm_clf = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    nb_clf = MultinomialNB()\n",
    "\n",
    "    # Create ensemble model\n",
    "    ensemble_clf = VotingClassifier(\n",
    "        estimators=[('SVM', svm_clf), ('RF', rf_clf), ('NB', nb_clf)],\n",
    "        voting='soft'  # Use 'hard' for majority vote\n",
    "    )\n",
    "\n",
    "    # Train ensemble model\n",
    "    ensemble_clf.fit(train_X, train_y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predicted_y = ensemble_clf.predict(test_X)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(test_y, predicted_y)\n",
    "    print(f\"Accuracy with {description} (Ensemble): {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_y, predicted_y)\n",
    "    print(f\"Confusion Matrix for {description} (Ensemble):\\n{cm}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_y, predicted_y))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on word level Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Word-Level Features (Ensemble): 0.6720\n",
      "Confusion Matrix for Word-Level Features (Ensemble):\n",
      "[[ 36  52  28]\n",
      " [ 15 302  69]\n",
      " [ 13  92 213]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.56      0.31      0.40       116\n",
      "         neu       0.68      0.78      0.73       386\n",
      "         pos       0.69      0.67      0.68       318\n",
      "\n",
      "    accuracy                           0.67       820\n",
      "   macro avg       0.64      0.59      0.60       820\n",
      "weighted avg       0.66      0.67      0.66       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Word-Level Features with Ensemble\n",
    "word_featuresets = [(document_features(d, word_features), c) for (d, c) in train_documents]\n",
    "accuracy_ensemble_word = evaluate_ensemble(word_featuresets, \"Word-Level Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6719512195121952"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ensemble_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on bigram level Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bigram Features (Ensemble): 0.4707\n",
      "Confusion Matrix for Bigram Features (Ensemble):\n",
      "[[  0 116   0]\n",
      " [  0 386   0]\n",
      " [  0 318   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00       116\n",
      "         neu       0.47      1.00      0.64       386\n",
      "         pos       0.00      0.00      0.00       318\n",
      "\n",
      "    accuracy                           0.47       820\n",
      "   macro avg       0.16      0.33      0.21       820\n",
      "weighted avg       0.22      0.47      0.30       820\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saikumarreddypochireddygari/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saikumarreddypochireddygari/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saikumarreddypochireddygari/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Bigram Features with Ensemble\n",
    "bigram_featuresets = [(bigram_document_features(d, bigram_features), c) for (d, c) in train_documents]\n",
    "accuracy_ensemble_bigram = evaluate_ensemble(bigram_featuresets, \"Bigram Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47073170731707314"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ensemble_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on word + bigram + trigram Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Combined Features (Ensemble): 0.6732\n",
      "Confusion Matrix for Combined Features (Ensemble):\n",
      "[[ 46  39  31]\n",
      " [ 19 296  71]\n",
      " [ 18  90 210]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.55      0.40      0.46       116\n",
      "         neu       0.70      0.77      0.73       386\n",
      "         pos       0.67      0.66      0.67       318\n",
      "\n",
      "    accuracy                           0.67       820\n",
      "   macro avg       0.64      0.61      0.62       820\n",
      "weighted avg       0.67      0.67      0.67       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Combined Features with Ensemble\n",
    "combined_featuresets = [\n",
    "    (combined_features(d, word_features, bigram_features, trigram_features), c)\n",
    "    for (d, c) in train_documents\n",
    "]\n",
    "accuracy_ensemble_combined = evaluate_ensemble(combined_featuresets, \"Combined Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6731707317073171"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ensemble_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on bigram plus pos features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bigram + POS features (Ensemble): 0.6573\n",
      "Confusion Matrix for Bigram + POS features (Ensemble):\n",
      "[[ 14  73  29]\n",
      " [  3 312  71]\n",
      " [  4 101 213]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.12      0.20       116\n",
      "         neu       0.64      0.81      0.72       386\n",
      "         pos       0.68      0.67      0.68       318\n",
      "\n",
      "    accuracy                           0.66       820\n",
      "   macro avg       0.66      0.53      0.53       820\n",
      "weighted avg       0.66      0.66      0.63       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6573170731707317"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_ensemble(bigram_pos_featuresets, \"Bigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on trigram + pos Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Trigram + POS features (Ensemble): 0.6610\n",
      "Confusion Matrix for Trigram + POS features (Ensemble):\n",
      "[[ 12  68  36]\n",
      " [  4 319  63]\n",
      " [  4 103 211]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.60      0.10      0.18       116\n",
      "         neu       0.65      0.83      0.73       386\n",
      "         pos       0.68      0.66      0.67       318\n",
      "\n",
      "    accuracy                           0.66       820\n",
      "   macro avg       0.64      0.53      0.53       820\n",
      "weighted avg       0.66      0.66      0.63       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6609756097560976"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_ensemble(trigram_pos_featuresets, \"Trigram + POS features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. Performing evaluation using voting Ensemble model from sklearn.ensemble package (using SVC, RF, NB) from Sklearn Class on word + bigram + trigram Features #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Combined features (Ensemble): 0.6732\n",
      "Confusion Matrix for Combined features (Ensemble):\n",
      "[[ 46  39  31]\n",
      " [ 19 296  71]\n",
      " [ 18  90 210]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.55      0.40      0.46       116\n",
      "         neu       0.70      0.77      0.73       386\n",
      "         pos       0.67      0.66      0.67       318\n",
      "\n",
      "    accuracy                           0.67       820\n",
      "   macro avg       0.64      0.61      0.62       820\n",
      "weighted avg       0.67      0.67      0.67       820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6731707317073171"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate_ensemble(combined_featuresets, \"Combined features\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on Further Analysis - Vaishnavi Meka ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I tried different models (NB, SVC, RF, DT, SVC+RF+NB) and tried to mix and match the models with different featuresets like word, bigrams, trigrams, Bigrams with POS, tri-grams with POS, Word+bigrams+trigrams featuresets.\n",
    "\n",
    "2. Out of all the features & Models used, I find that Ensemble model gave the higest accuracy like 67%. This has the featuresets: Word, bigrams, trigrams combined. \n",
    "\n",
    "3. Out of all the features & Models used, I find that Ensemble model gave the higest accuracy like 67%. This has the featuresets: Word Level.\n",
    "\n",
    "4. Over all Ensemble models did well with out any hyper parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope of work - Vaishnavi Meka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Futue scope of work can be using DL Neural network based models\n",
    "2. Using different hyper parameter optimization techniques for tunning the hyper parameters\n",
    "3. Using more complex feature engineering techniques like TF-IDF, Word embeddings, GloVe, FastText etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
